---
title: "Escalamiento multidimensional no métrico"
output:
  pdf_document: default
  html_document: default
date: "2024-04-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Librerias necesarias

```{r}
library(car)
library(smacof)
library(cluster)
library(lubridate)
library(andrews)
library(dplyr)
library(corrplot)
source("utilerias/funciones.R")
```


# Introducción:

Se hace un análisis de escalamiento multidimensional, seleccionaremos la mejor dimensión con las calificaciones o los ratings (columnas 5:18) de la base de datos ``RockHard`` del paquete ``smacof``. Los datos son de la revista RockHard, una revista alemana de heavy metal, los redactores valoran cada mes alrededor de 50 discos en una escala de (0 como peor a 10 como mejor), el conjunto de datos contiene todas las calificaciones de 2013. Los redactores o evaluadores en las columnas, y las bandas/álbumes en las filas.



El escalamiento multidimensional no métrico tiene por objetivo preservar las disimilaridades mientras se posicionan los objetos en una menor dimension. Se aplica principalmente sobre datos ordinales, en este caso las calificaciones o ratings para las bandas y album. 

Pasos escenciales:

0.- Preprocesamiento (Datos atípicos, escalamiento, transformaciones)

1.- Matriz de datos con escala ordinales (Definir rangos)

2.- Cálculo de disimilaridades

4.- Escalamiento no métrico (Regresión monótona)

5.- Mejoras



# Carga de la informacion

En esta práctica analizaremos  los ratings de bandas y álbunes de música de la revista RockHard para los 12 meses del año 2013, un album por cada banda. 


Las variables categóricos ordinales son las calificaciones para las bandas y sus álbunes dados por 14 personas; Gotz, Thomas, Frank, Bjorn, Jan, Boris, Himmelstein, Michael, Jens, Ronny, Felix, Jakob, Marcus y Jenny. 

Para tener una idea general de la base de datos, mostramos los primeros 10 bandas y álbunes. 


```{r}
data1 <- smacof::RockHard
head(data1, 10)
```



# Formato Correcto

Primero creamos la variable Band_Album y luego la establecemos como índice de la base de datos. Quitamos variables que no usaremos en el análisis. Se muestran las primeras 10 observaciones. 



```{r}
data1$Band_Album<-paste(data1$Band,"_",data1$Album)
```



```{r}
rownames(data1) <- data1$Band_Album # Estableciendo como indice las bandas
data1$Band_Album <- NULL # Estableciendo como indice las bandas
data1 <- subset(data1, select = -c(Year, Month, Band, Album))
head(data1)
```




# Seleccion de las columnas auxiliares y de analisis.

En esta sección analizamos algunos datos y generaremos otros. Por ejemplo definimos la variable ``avrating`` del promedio de los ratings de todos los participantes. Como hay muchos valores faltantes (NA's), primero hacenmos una revisión general para cada participante. 


Podemos observar en la siguiente salida que los participantes que tienen más del 50% de NA's, son Felix y Jenny que tienen 478 y 383 NA'S de las 576 observaciones de la base de datos, respectivamente.  


```{r}
sprintf("Gotz: %d de %d", sum(is.na(data1$Götz)),  576)
sprintf("Thomas: %d de %d", sum(is.na(data1$Thomas)),  576)
sprintf("Frank: %d de %d", sum(is.na(data1$Frank)),  576)
sprintf("Björn: %d de %d", sum(is.na(data1$Björn)),  576)
sprintf("Jan: %d de %d", sum(is.na(data1$Jan)),  576)
sprintf("Boris: %d de %d", sum(is.na(data1$Boris)),  576)
sprintf("Himmelstein: %d de %d", sum(is.na(data1$Himmelstein)),  576)
sprintf("Michael: %d de %d", sum(is.na(data1$Michael)),  576)
sprintf("Jens: %d de %d", sum(is.na(data1$Jens)),  576)
sprintf("Ronny: %d de %d", sum(is.na(data1$Ronny)),  576)
sprintf("Felix: %d de %d", sum(is.na(data1$Felix)),  576)
sprintf("Jakob: %d de %d", sum(is.na(data1$Jakob)),  576)
sprintf("Marcus: %d de %d", sum(is.na(data1$Marcus)),  576)
sprintf("Jenny: %d de %d", sum(is.na(data1$Jenny)),  576)
```




Observemos en la siguiente tabla de correlaciones, que todas están en los mismos órdenes, por lo que tomaremos las primeras 7 variables y omitiremos las últimas 7, pues generan muchos problemas al tener NA's y excluirlas no afecta realmente las relaciones. 




```{r}
options(digits=2)
cor(data1,  method = "pearson", use = "pairwise.complete.obs")
```


En la siguiente tabla se muestran estas mismas correlaciones para las variables elegidas (participantes). 

```{r}
data1<-subset(data1, select = -c(Michael, Jens, Ronny, Felix, Jakob, Marcus, Jenny))
```


```{r}
options(digits=2)
cor(data1,  method = "pearson")
```






Decidimos crear la variable ``avrating``, que es el promedio de todas las calificaciones asignadas por todos los 7 participantes, considerando que hay NA's.Además, generamos la variable ``grado``, que es el grado medido como bajo (0 a 4),  medio (4 a 6), alto (6 a 8) y muy alto (8 a 10). 




```{r}
data1$avrating <- rowMeans(data1[,1:7], na.rm=TRUE)
```


```{r}
data1<-data1 %>% mutate(grado = case_when(avrating <= 4 ~ "Bajo", 
                                   avrating <= 6 ~ "Medio",
                                   avrating <= 8 ~ "Alto", 
                                   avrating <= 10 ~ "Muy alto"))
head(data1,10)
```



Estableceremos la escala ordinal para la variable de grado.


```{r}
# Establecimiendo de escalas ordinales
data1$grado <- factor(data1$grado, levels= c("Bajo","Medio","Alto","Muy alto"), order=TRUE)
```


Luego indicamos las variables auxiliares y las variables de análisis (las que entran directamente en los cálculos numéricos), y juntamos toda la información a utilizar en el análisis en una sola base de datos (la matriz de información). 


```{r}
auxiliares <- colnames(data1[, c(8,9)])
analisis <- colnames(data1[,1:7]) # Seleccion de columnas
columnas <- c(auxiliares, analisis)
datos <- data1[, columnas] # Extraccion
```



# Escalas Iniciales


Verificamos y asignamos las escalas iniciales, y el tipo de variables entre numéricas contínuas y enteras, o categóricas nominales y ordinales. 


```{r}
# Escalas
tipo <- sapply(datos, class)
continuas <-  which(tipo == "numeric") # continuas
enteras <- which(tipo == "integer") # enteras
numericas <- names(c(continuas,enteras))

# Variables Categóricas
nominales <- which( tipo == "factor") # categóricas
ordinales <- which( sapply(datos, is.ordered) )  # ordinales
fecha <- which(tipo == "Date") # Fecha
categoricas <- names(c(nominales, ordinales, fecha))
```



# Descriptivos Multivariados

* Identificar Atípicos

* Problemas de escala

* Distribuciones


A continuación mostramos el histograma de la variable promedio creada a partir de los ratings o calificaciones de todos, y los histogramas de  de cada uno de los 7 participantes o redactores que asignan la calificación.




```{r}
# Histogramas
par(mfrow= c(2,2) )
multi.hist(datos[, numericas])
```

En el siguiente BoxPlot no parece haber problemas de escala con las calificaciones. También se puede observar la variable de grado, en el segundo Boxplot, que es categórica ordinal. 



```{r}
# Boxplot
boxplot(datos, main="Caja y Bigotes",
        frame = FALSE, xlab="Variables", ylab= "Escala Normal", cex=0.4);grid()
```



El Gráfico Andrews, es una forma de representar datos de dimensiones superiores en un espacio inferior para que el análisis sea sencillo. Se representa mediante una serie de Fourier y también se puede considerar como una proyección de puntos de datos de dimensiones superiores en un vector dinámico.



```{r}
# Andrews
andrews(df = datos, type=2, bty = "n", ylab="f(t)", xlab="t",lwd=1, main="Grafico Andrews" ); grid()
```



# Eliminacion de datos atipicos

* Importante ver que la variable auxiliar ayuda a identificar observaciones que afecten el análisis.

En el siguiente BoxPlot se observan algunos outliers por debajo del primer quartil, sin embargo NO quitaremos esta información porque quita representatividad a la clasificación de "bajo". 



```{r}
outliers <- boxplot(datos$avrating)$out
elementos <- which(datos$avrating %in% outliers)
#datos <- datos[-union(elementos,elementos), ]
```



# Escalamiento

En general, es importante recordar que los indices se recodifican a una escala ordinal, pero primero se normalizan. Sin embargo, en la base de datos que estamos analizando tenemos calificaciones de 0 a 10, no parece haber un problema grave de escalas, por lo que no sería necesario normalizar. Pero, vamos a normalizar para tenerlos en escala de 0 a 1, lo cual no es necesario, pero lo trabajaremos así ya que estamos tratando con distancias.    


```{r}
# Normalizacion
datos[,analisis] <- sapply(datos[, analisis], function(data){
         (data - min(data, na.rm = TRUE)) / (max(data, na.rm = TRUE) - min(data, na.rm = TRUE))})
# Boxplot
boxplot(datos[, analisis], main="Caja y Bigotes",
        frame = FALSE, xlab="Variables", ylab= "Escala Normal", cex=0.4);grid()
```

# 1 Matriz de datos con escala ordinales

Definicion de rangos para la escala de lickert, con las siguientes categorías, para poder utilizar escalamiento no métrico con las variables de análisis.  Recordemos que nos interesan las categorías ordinales de la variable ``grado`` de tipo bajo, medio, alto y mul alto.  Se escalan al nivel 0 a 100, y se redondean para poder aplicar el analisis.  

0-40 -> 1

40:60 -> 2

61:80 -> 3

81:100 -> 4





```{r}
# Transformacion a escala ordinal
datos[, analisis] <- datos[, analisis]*100
datos[, analisis] <- round(datos[, analisis])

for(indice in analisis){
  for(n in 1:nrow(datos)){
    datos[n,indice] = car::recode(datos[n,indice], "0:40=1; 41:60=2; 61:80=3; 81:100=4")
  }
} 

# Formato Correcto
for(indice in analisis){
  datos[, indice] <- factor(datos[, indice], order = TRUE)
}

# Redefinicion de Escalas
tipo <- sapply(datos, class)
continuas <-  which(tipo == "numeric") # continuas
enteras <- which(tipo == "integer") # enteras
numericas <- names(c(continuas,enteras))

# Variables Categoricas
nominales <- which( tipo == "factor") # categoricas
ordinales <- which( sapply(datos, is.ordered) )  # ordinales
fecha <- which(tipo == "Date") # Fecha
categoricas <- names(c(nominales, ordinales, fecha))
```






# Calculo de la matriz de Disimilaridad

* Como las variables son categóricas en escala ordinal, entonces se utiliza distancia gower(mixtas), para tener la matriz de disimilarides a través de variables categóricas ordinales ya escaladas y ordenas de manera adecuada para que funcione el algoritmo de ``daisy``.     

```{r}
gower_dist <- daisy(datos[, analisis], metric = "gower")
```




# Escalamiento no métrico

* Métricas de ajuste:  Sabemos que  una regresión monótona, tiene ciertas métricas de ajuste, como los siguientes.  En este caso, a prueba y error se encontro que 7 es la mejor dimensión, pues conforme disminuimos la dimensión los valores de estas variables de ajuste. 

Stress: con valor entre [0,1] y entre mas pequeño (más cercano a cero) es mejor. 

RSS: entre mas pequeño es mejor.  


```{r}
fit.datos <- smacofSym(gower_dist, type = "ordinal", ndim = 7)
Stress<-fit.datos$stress
sprintf("Stress: %f", Stress)
RSS<-fit.datos$rss
sprintf("RSS: %f", RSS)

```



En la siguiente Gráfica se muestra la dispersión, en donde se quiere observar la banda y album con el tipo de grado que tiene en cuanto a la calificación promedio. 


```{r}
# Dispersion
plot(fit.datos, plot.dim = c(1,2), main = "Escalamiento Multidimensional No metrico", 
     xlab="Dim 1", ylab="Dim 2", cex=0.5, cex.main=1, 
      bty = "n",  col = datos$Grado.de.rezago.social );grid()
```


En la siguiente Gráfica se muestra la curva Shepard, las franjas grises son un poco anchas, es decir, se tienen RSS algo grandes, lo cual habla de un ajuste de la regresión monótona no tan bueno.  


```{r}
# Curva Shape
plot(fit.datos, plot.type = "Shepard", main="Curva Shepard",
     xlab="Distancias observadas", ylab="Configuracion de distancias", cex=0.5, cex.main=1,
     col="skyblue", bty = "n");grid()
```




# Mejoras

Para mejorar el ajuste, se puede intentar los siguiente:

1. Incrementar el numero de dimensiones(Capturar mayor variabilidad que implica menor rss)

2. Usar otra medida de disimilaridad

3. Usar otro algoritmo de optimizacion para el escalamiento

4. Problemas de preprocesamiento

5. Usar otro metodo como t-sne


# Implementación de t-sne



```{python Modulos}
from sklearn.manifold import TSNE
import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
```

# Implementacion

```{python Datos  Variables suplementarias}
# Datos
datos = r.datos

# Particion horizontal
x = np.array(datos[r.analisis])
y = np.array(datos[r.auxiliares[1]]) # Variable suplementaria
```

# Ajuste

```{python Variables  t-SNE}
x_coord = TSNE(n_components = 3, perplexity = 30, n_iter = 4000).fit_transform(x)
```

# Grafico

```{python t-SNE Grafico}
plt.clf()
sns.set(style="whitegrid")
sns.relplot(x=x_coord[:,0], y=x_coord[:,1], hue=y, palette="muted" )
plt.show()
exit
```





